{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b53d9a-c7e1-45d6-9fa6-41d1c7a90aef",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "49b53d9a-c7e1-45d6-9fa6-41d1c7a90aef"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Compose, Lambda\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2829922e-0a81-4dc0-a969-3aca9df8bc5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2829922e-0a81-4dc0-a969-3aca9df8bc5c",
        "outputId": "6e4c1e18-f69e-4e54-91c6-3c60d013b8f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 58.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.76MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.4MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.80MB/s]\n"
          ]
        }
      ],
      "source": [
        "# I hear adding noise to the data improves generalization!\n",
        "# it's called regularization! I learned it in the lecture\n",
        "train_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=Compose([ToTensor(), Lambda(lambda x: x + torch.distributions.Uniform(-5., 5.).sample(x.shape))]))\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b56744fd-68e7-41b2-8f6b-b01287d584c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b56744fd-68e7-41b2-8f6b-b01287d584c6",
        "outputId": "99971e64-a8a1-4be7-dc8f-446134991542"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([128, 1, 28, 28])\n",
            "Shape of y: torch.Size([128]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True,\n",
        "                              num_workers=8)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size,\n",
        "                             num_workers=8)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d28ea3d9-1837-4cbc-8604-753636015046",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d28ea3d9-1837-4cbc-8604-753636015046",
        "outputId": "af2e61de-6fbc-4513-f4e3-a38d2897ed4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (a0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (b0): ReLU()\n",
            "  (a1): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (b1): ReLU()\n",
            "  (final): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "n_layers = 2\n",
        "hidden_dim = 128\n",
        "input_dim = 784\n",
        "model = nn.Sequential(nn.Flatten())\n",
        "for ind in range(n_layers):\n",
        "    model.add_module(\"a\" + str(ind), nn.Linear(input_dim if ind == 0 else hidden_dim, hidden_dim))\n",
        "    model.add_module(\"b\" + str(ind), nn.ReLU())\n",
        "model.add_module(\"final\", nn.Linear(hidden_dim, 10))\n",
        "\n",
        "model = model.to(device)\n",
        "print(model)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.03)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc66f666-6ea2-4e66-bef8-175cadd13b39",
      "metadata": {
        "id": "dc66f666-6ea2-4e66-bef8-175cadd13b39"
      },
      "outputs": [],
      "source": [
        "def train_model(model: nn.Module,\n",
        "                loss_fn: nn.Module,\n",
        "                optimizer: torch.optim.Optimizer,\n",
        "                training_loader: DataLoader,\n",
        "                validation_loader: DataLoader,\n",
        "                n_epochs: int,\n",
        "                verbose: bool = True):\n",
        "    n_training_examples = len(training_loader.dataset)\n",
        "    batches_per_epoch = n_training_examples // training_loader.batch_size\n",
        "    print(\"Running {} epochs at {} steps per epoch.\".format(n_epochs, batches_per_epoch))\n",
        "\n",
        "    # note, for training we only track the average over the epoch.\n",
        "    # this is somewhat imprecise, as the model changes over the epoch.\n",
        "    # so the metrics at the end of the epoch will usually be better than at the start,\n",
        "    # but we average over everything.\n",
        "    # we could record train metrics more often to get a better picture of training progress.\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        if verbose:\n",
        "            print(\"Starting epoch {}...\".format(epoch + 1), end=\" \")\n",
        "\n",
        "        start_time = perf_counter()\n",
        "        epoch_train_losses = []\n",
        "        epoch_train_accuracies = []\n",
        "\n",
        "        model.train()\n",
        "        for batch_ind, (input_batch, label_batch) in enumerate(training_loader):\n",
        "            batch_loss, batch_accuracy = train_step(input_batch, label_batch, model, loss_fn, optimizer)\n",
        "            epoch_train_losses.append(batch_loss.item())\n",
        "            epoch_train_accuracies.append(batch_accuracy.item())\n",
        "\n",
        "        end_time = perf_counter()\n",
        "        time_taken = end_time - start_time\n",
        "\n",
        "        # evaluate after each epoch\n",
        "        val_loss, val_accuracy = evaluate(model, validation_loader, loss_fn)\n",
        "\n",
        "        val_losses.append(val_loss.item())\n",
        "        val_accuracies.append(val_accuracy.item())\n",
        "        train_losses.append(np.mean(epoch_train_losses))\n",
        "        train_accuracies.append(np.mean(epoch_train_accuracies))\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Time taken: {} seconds\".format(time_taken))\n",
        "            print(\"\\tTrain/val loss: {} / {}\".format(train_losses[-1], val_losses[-1]))\n",
        "            print(\"\\tTrain/val accuracy: {} / {}\".format(train_accuracies[-1], val_accuracies[-1]))\n",
        "\n",
        "    return {\"train_loss\": np.array(train_losses), \"train_accuracy\": np.array(train_accuracies),\n",
        "            \"val_loss\": np.array(val_losses), \"val_accuracy\": np.array(val_accuracies)}\n",
        "\n",
        "\n",
        "def train_step(input_batch: torch.tensor,\n",
        "               label_batch: torch.tensor,\n",
        "               model: nn.Module,\n",
        "               loss_fn: nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "    input_batch = input_batch.to(device)\n",
        "    label_batch = label_batch.to(device)\n",
        "    output_batch = model(input_batch)\n",
        "    batch_loss = loss_fn(output_batch, label_batch)\n",
        "\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        batch_accuracy = accuracy(label_batch, output_batch)\n",
        "    return batch_loss, batch_accuracy\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             dataloader: DataLoader,\n",
        "             loss_fn: nn.Module):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    val_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_batch, label_batch in dataloader:\n",
        "            input_batch = input_batch.to(device)\n",
        "            label_batch = label_batch.to(device)\n",
        "            predictions = model(input_batch)\n",
        "            val_loss += loss_fn(predictions, label_batch)\n",
        "            correct += (predictions.argmax(axis=1) == label_batch).type(torch.float).sum()\n",
        "\n",
        "        val_loss /= num_batches\n",
        "        val_accuracy = correct / size\n",
        "    return val_loss, val_accuracy\n",
        "\n",
        "\n",
        "def accuracy(labels: torch.tensor,\n",
        "             outputs: torch.tensor) -> torch.tensor:\n",
        "    predictions = torch.argmax(outputs, axis=-1)\n",
        "    matches = labels == predictions\n",
        "    return matches.float().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f588602c-523f-47a7-a1ef-74381a0b95e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f588602c-523f-47a7-a1ef-74381a0b95e5",
        "outputId": "9f6da57f-d0be-40e1-c4a5-a3f2dae23bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running 15 epochs at 468 steps per epoch.\n",
            "Starting epoch 1... Time taken: 19.704791262000015 seconds\n",
            "\tTrain/val loss: 2.196601983573702 / 1.7648091316223145\n",
            "\tTrain/val accuracy: 0.19766626602564102 / 0.6640999913215637\n",
            "Starting epoch 2... Time taken: 18.082438747000026 seconds\n",
            "\tTrain/val loss: 1.931831603630995 / 1.4513981342315674\n",
            "\tTrain/val accuracy: 0.31547142094017094 / 0.7355999946594238\n",
            "Starting epoch 3... Time taken: 19.866539338999985 seconds\n",
            "\tTrain/val loss: 1.877176999536335 / 1.3872276544570923\n",
            "\tTrain/val accuracy: 0.33842481303418803 / 0.7653999924659729\n",
            "Starting epoch 4... Time taken: 18.624165148999964 seconds\n",
            "\tTrain/val loss: 1.8575618534516065 / 1.3524956703186035\n",
            "\tTrain/val accuracy: 0.35067775106837606 / 0.7914000153541565\n",
            "Starting epoch 5... Time taken: 19.594487929999957 seconds\n",
            "\tTrain/val loss: 1.849320159253911 / 1.3355538845062256\n",
            "\tTrain/val accuracy: 0.3540831997863248 / 0.7961999773979187\n",
            "Starting epoch 6... Time taken: 18.32497814300001 seconds\n",
            "\tTrain/val loss: 1.8374539766556177 / 1.3464233875274658\n",
            "\tTrain/val accuracy: 0.35738848824786323 / 0.7985000014305115\n",
            "Starting epoch 7... Time taken: 19.868122052000047 seconds\n",
            "\tTrain/val loss: 1.8318981619981618 / 1.3165591955184937\n",
            "\tTrain/val accuracy: 0.3597255608974359 / 0.8033999800682068\n",
            "Starting epoch 8... Time taken: 18.06771179399999 seconds\n",
            "\tTrain/val loss: 1.8295461318941197 / 1.3161019086837769\n",
            "\tTrain/val accuracy: 0.36010950854700857 / 0.7885000109672546\n",
            "Starting epoch 9... Time taken: 19.558802415999992 seconds\n",
            "\tTrain/val loss: 1.826225643738722 / 1.322155237197876\n",
            "\tTrain/val accuracy: 0.360927483974359 / 0.8137000203132629\n",
            "Starting epoch 10... Time taken: 18.201733512999965 seconds\n",
            "\tTrain/val loss: 1.823640544954528 / 1.3186094760894775\n",
            "\tTrain/val accuracy: 0.3622963408119658 / 0.8277000188827515\n",
            "Starting epoch 11... Time taken: 19.44589642400001 seconds\n",
            "\tTrain/val loss: 1.8236996546769753 / 1.3153598308563232\n",
            "\tTrain/val accuracy: 0.3619624732905983 / 0.8292999863624573\n",
            "Starting epoch 12... Time taken: 18.364138673000014 seconds\n",
            "\tTrain/val loss: 1.816829253720422 / 1.3171478509902954\n",
            "\tTrain/val accuracy: 0.3643663194444444 / 0.8226000070571899\n",
            "Starting epoch 13... Time taken: 19.649042108999993 seconds\n",
            "\tTrain/val loss: 1.8108796637791853 / 1.3033232688903809\n",
            "\tTrain/val accuracy: 0.36708733974358976 / 0.8547999858856201\n",
            "Starting epoch 14... Time taken: 18.133255379999923 seconds\n",
            "\tTrain/val loss: 1.8092380848705258 / 1.2923005819320679\n",
            "\tTrain/val accuracy: 0.3679553952991453 / 0.8305000066757202\n",
            "Starting epoch 15... Time taken: 19.889595139999983 seconds\n",
            "\tTrain/val loss: 1.8116802955285096 / 1.28957998752594\n",
            "\tTrain/val accuracy: 0.3665698450854701 / 0.838699996471405\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'train_loss': array([2.19660198, 1.9318316 , 1.877177  , 1.85756185, 1.84932016,\n",
              "        1.83745398, 1.83189816, 1.82954613, 1.82622564, 1.82364054,\n",
              "        1.82369965, 1.81682925, 1.81087966, 1.80923808, 1.8116803 ]),\n",
              " 'train_accuracy': array([0.19766627, 0.31547142, 0.33842481, 0.35067775, 0.3540832 ,\n",
              "        0.35738849, 0.35972556, 0.36010951, 0.36092748, 0.36229634,\n",
              "        0.36196247, 0.36436632, 0.36708734, 0.3679554 , 0.36656985]),\n",
              " 'val_loss': array([1.76480913, 1.45139813, 1.38722765, 1.35249567, 1.33555388,\n",
              "        1.34642339, 1.3165592 , 1.31610191, 1.32215524, 1.31860948,\n",
              "        1.31535983, 1.31714785, 1.30332327, 1.29230058, 1.28957999]),\n",
              " 'val_accuracy': array([0.66409999, 0.73559999, 0.76539999, 0.79140002, 0.79619998,\n",
              "        0.7985    , 0.80339998, 0.78850001, 0.81370002, 0.82770002,\n",
              "        0.82929999, 0.82260001, 0.85479999, 0.83050001, 0.8387    ])}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_model(model, loss_fn, optimizer, train_dataloader, test_dataloader, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27605351-7518-4560-bf57-f8d93b9cd858",
      "metadata": {
        "id": "27605351-7518-4560-bf57-f8d93b9cd858"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e216e94-d879-4081-9925-89da3d6f5dc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "7e216e94-d879-4081-9925-89da3d6f5dc9",
        "outputId": "391a4203-0543-41e3-f40b-266dbc34a458"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f3f9248dd940>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_activation_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "activation_stats = defaultdict(list)\n",
        "\n",
        "def get_activation_hook(name):\n",
        "    def hook(model, input, output):\n",
        "        act = output.detach()\n",
        "        activation_stats[name].append({\n",
        "            \"mean\": act.mean().item()\n",
        "        })\n",
        "    return hook\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Sigmoid):\n",
        "        module.register_forward_hook(get_activation_hook(name))\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from typing import Optional\n",
        "\n",
        "def train_model(model: nn.Module,\n",
        "                loss_fn: nn.Module,\n",
        "                optimizer: torch.optim.Optimizer,\n",
        "                training_loader: DataLoader,\n",
        "                validation_loader: DataLoader,\n",
        "                n_epochs: int,\n",
        "                verbose: bool = True,\n",
        "                logdir: Optional[str] = None):\n",
        "    n_training_examples = len(training_loader.dataset)\n",
        "    batches_per_epoch = n_training_examples // training_loader.batch_size\n",
        "    print(\"Running {} epochs at {} steps per epoch.\".format(n_epochs, batches_per_epoch))\n",
        "\n",
        "    # note, for training we only track the average over the epoch.\n",
        "    # this is somewhat imprecise, as the model changes over the epoch.\n",
        "    # so the metrics at the end of the epoch will usually be better than at the start,\n",
        "    # but we average over everything.\n",
        "    # we could record train metrics more often to get a better picture of training progress.\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "\n",
        "    writer = SummaryWriter(logdir)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        if verbose:\n",
        "            print(\"Starting epoch {}...\".format(epoch + 1), end=\" \")\n",
        "\n",
        "        start_time = perf_counter()\n",
        "        epoch_train_losses = []\n",
        "        epoch_train_accuracies = []\n",
        "\n",
        "        model.train()\n",
        "        for batch_ind, (input_batch, label_batch) in enumerate(training_loader):\n",
        "\n",
        "            # Total Step\n",
        "            total_step = batch_ind + batches_per_epoch * epoch\n",
        "\n",
        "            batch_loss, batch_accuracy = train_step(input_batch, label_batch, model, loss_fn, optimizer, writer, total_step)\n",
        "            epoch_train_losses.append(batch_loss.item())\n",
        "            epoch_train_accuracies.append(batch_accuracy.item())\n",
        "\n",
        "        end_time = perf_counter()\n",
        "        time_taken = end_time - start_time\n",
        "\n",
        "        # evaluate after each epoch\n",
        "        val_loss, val_accuracy = evaluate(model, validation_loader, loss_fn, writer, total_step)\n",
        "\n",
        "        val_losses.append(val_loss.item())\n",
        "        val_accuracies.append(val_accuracy.item())\n",
        "        train_losses.append(np.mean(epoch_train_losses))\n",
        "        train_accuracies.append(np.mean(epoch_train_accuracies))\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Time taken: {} seconds\".format(time_taken))\n",
        "            print(\"\\tTrain/val loss: {} / {}\".format(train_losses[-1], val_losses[-1]))\n",
        "            print(\"\\tTrain/val accuracy: {} / {}\".format(train_accuracies[-1], val_accuracies[-1]))\n",
        "\n",
        "            writer.add_scalar(\"epoch_train_loss\", batch_loss.item(), total_step)\n",
        "\n",
        "    return {\"train_loss\": np.array(train_losses), \"train_accuracy\": np.array(train_accuracies),\n",
        "            \"val_loss\": np.array(val_losses), \"val_accuracy\": np.array(val_accuracies)}\n",
        "\n",
        "\n",
        "def train_step(input_batch: torch.tensor,\n",
        "               label_batch: torch.tensor,\n",
        "               model: nn.Module,\n",
        "               loss_fn: nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               writer: SummaryWriter,\n",
        "               total_step: int):\n",
        "\n",
        "    input_batch = input_batch.to(device)\n",
        "    label_batch = label_batch.to(device)\n",
        "    output_batch = model(input_batch)\n",
        "\n",
        "    batch_loss = loss_fn(output_batch, label_batch)\n",
        "\n",
        "    writer.add_scalar(\"step_train_loss\", batch_loss.item(), total_step)\n",
        "\n",
        "    batch_loss.backward()\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            writer.add_scalars(f'gradients_{name}', {\n",
        "                        'mean': param.grad.mean().item()\n",
        "                    }, total_step)\n",
        "            writer.add_histogram(f'gradients_{name}', param.grad, total_step)\n",
        "\n",
        "    with torch.no_grad():\n",
        "            for name, parameter in model.named_parameters():\n",
        "                writer.add_scalar(\"gradient_\" + name, torch.sqrt((parameter.grad**2).sum()), total_step)\n",
        "                writer.add_histogram(name, parameter, total_step)\n",
        "            if total_step % 100 == 0:\n",
        "              writer.add_images(\"images/train\", input_batch[:1], total_step)\n",
        "\n",
        "\n",
        "    for layer_name, stats in activation_stats.items():\n",
        "        if len(stats) > 0:\n",
        "            last = stats[-1]  # Most recent batch stats\n",
        "            writer.add_scalars(f'activations_{layer_name}', {\n",
        "                'mean': last['mean']\n",
        "            }, total_step)\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        batch_accuracy = accuracy(label_batch, output_batch)\n",
        "    return batch_loss, batch_accuracy\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             dataloader: DataLoader,\n",
        "             loss_fn: nn.Module,\n",
        "             writer: SummaryWriter,\n",
        "             total_step: int):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    val_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_batch, label_batch in dataloader:\n",
        "            input_batch = input_batch.to(device)\n",
        "            label_batch = label_batch.to(device)\n",
        "            predictions = model(input_batch)\n",
        "            val_loss += loss_fn(predictions, label_batch)\n",
        "            correct += (predictions.argmax(axis=1) == label_batch).type(torch.float).sum()\n",
        "        if total_step % 100 == 0:\n",
        "              writer.add_images(\"images/validate\", input_batch[:1], total_step)\n",
        "\n",
        "        val_loss /= num_batches\n",
        "        val_accuracy = correct / size\n",
        "    return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vTZUuFkLpWQv",
      "metadata": {
        "id": "vTZUuFkLpWQv"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import ToTensor, Compose, Lambda\n",
        "# I hear adding noise to the data improves generalization!\n",
        "# it's called regularization! I learned it in the lecture\n",
        "train_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=Compose([ToTensor(), Lambda(lambda x: x + torch.distributions.Uniform(-0.1, 0.1).sample(x.shape))]))\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
